<h1 id="gradient-decent">Gradient decent</h1>

<hr>

<h2 id="linear-regression">linear regression</h2>

<p><strong>hypothesis</strong> <br>
<script type="math/tex; mode=display" id="MathJax-Element-25">h_{\theta}(x) = \sum_{i=1}^n\theta_ix_i</script></p>

<p><em>vectorize</em>    <script type="math/tex; mode=display" id="MathJax-Element-26">X*\theta</script> <br>
<strong>cost function</strong> <br>
<script type="math/tex; mode=display" id="MathJax-Element-27">J(\theta) = \frac{1}{2m}\sum_{i=1}^n(h_\theta(x^i)-y^i)^2</script> <br>
<strong>Gradient Descent</strong> <br>
<script type="math/tex; mode=display" id="MathJax-Element-28">\theta_j := \theta_j - \alpha\frac{\partial}{\partial \theta_j}J_\theta</script> <br>
<script type="math/tex; mode=display" id="MathJax-Element-29">gradient = \frac{\partial}{\partial \theta_j}J_\theta
=\frac{1}{m}\sum_{i=1}^n(h_\theta(x^i)-y^i)x^i_j</script></p>

<h2 id="logistic-regression">Logistic Regression</h2>

<p><strong>hypothesis</strong> <br>
<script type="math/tex; mode=display" id="MathJax-Element-18">h_\theta(x) = sigmoid(\theta*x) = \frac{1}{1+e^(x\theta)}=P{(y|x,\theta)}</script> <br>
<strong>cost function</strong> <br>
<script type="math/tex; mode=display" id="MathJax-Element-19">J(\theta) = -\frac{1}{m}\sum_{i=1}^n[y^ilog(h_\theta(x^i))+(1-y^i)log(1-h_\theta(x^i))]</script> <br>
<strong>Gradient Descent</strong></p>

<h2 id="regularization">Regularization</h2>

<p><em>optima</em> <br>
<em>Formal Equation</em></p>

<blockquote>
  <p>Written with <a href="https://stackedit.io/">StackEdit</a>.</p>
</blockquote>